{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 style='text-align: center;'>HousePrices Regression</h1><br>\n<p>Hello!<br><br>This is my first notebook. After studying some notebooks of this competition, I summarized the overall basic process of regression analysis. This kernel introduces the following.</p>\n<ol>\n    <li><b>Data EDA</b><br>\nThe process of searching for data is the most important step in analysis.<br>\nI will introduce how to define the type of each variable and how to identify and visualize the form.</li>\n    <li><b>Preprocessing</b><br>\nThis part is the step of preprocessing each variable based on the EDA results.<br>\nI will introduce preprocessing tasks such as outlier processing, missing value processing, derivative variable generation, and variable conversion.</li>\n    <li><b>Optimization (GridSearch, Optuna)</b><br>\nThe more complex the model is, the more complex the hyperparameter setting is required.<br>\nHyperparameter combinations can have a significant impact on the performance of the model.<br>\nI will introduce how to optimize the hyperparameters of the model using GridSearchCV and Optuna.</li>\n    <li><b>Modeling</b><br>\nThis part is the step of creating and learning basic models that can be used for regression analysis.<br>\nI will use linear regression models such as Lasso and Ridge, SVM, and some tree-based algorithms. And I will use Stacking to maximize generalization performance.</li>\n</ol>","metadata":{}},{"cell_type":"markdown","source":"<h2 style=\"text-align:center;\">Module import</h2>","metadata":{"execution":{"iopub.execute_input":"2021-10-22T01:14:06.102324Z","iopub.status.busy":"2021-10-22T01:14:06.101648Z","iopub.status.idle":"2021-10-22T01:14:06.129369Z","shell.execute_reply":"2021-10-22T01:14:06.128507Z","shell.execute_reply.started":"2021-10-22T01:14:06.102209Z"}}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 5000)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom scipy.stats import norm, skew, probplot\nfrom scipy.special import boxcox1p\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PolynomialFeatures, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_score\nfrom sklearn.feature_selection import SelectKBest, f_classif, chi2\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.linear_model import Lasso, Ridge, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge as krr\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor as rfr, GradientBoostingRegressor as gbr\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.cluster import DBSCAN\n\nimport optuna\nfrom functools import partial","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:20.826048Z","iopub.execute_input":"2021-10-23T18:02:20.826529Z","iopub.status.idle":"2021-10-23T18:02:20.83897Z","shell.execute_reply.started":"2021-10-23T18:02:20.82648Z","shell.execute_reply":"2021-10-23T18:02:20.837995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align:center;\">Load Data</h2>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ntest = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:20.840525Z","iopub.execute_input":"2021-10-23T18:02:20.840867Z","iopub.status.idle":"2021-10-23T18:02:20.895929Z","shell.execute_reply.started":"2021-10-23T18:02:20.840825Z","shell.execute_reply":"2021-10-23T18:02:20.895007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:20.897373Z","iopub.execute_input":"2021-10-23T18:02:20.897696Z","iopub.status.idle":"2021-10-23T18:02:20.960469Z","shell.execute_reply.started":"2021-10-23T18:02:20.897658Z","shell.execute_reply":"2021-10-23T18:02:20.959406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:20.962299Z","iopub.execute_input":"2021-10-23T18:02:20.96264Z","iopub.status.idle":"2021-10-23T18:02:21.029406Z","shell.execute_reply.started":"2021-10-23T18:02:20.962604Z","shell.execute_reply":"2021-10-23T18:02:21.028815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'train size: {train.shape}')\nprint(f'test size: {test.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:21.030622Z","iopub.execute_input":"2021-10-23T18:02:21.031012Z","iopub.status.idle":"2021-10-23T18:02:21.041789Z","shell.execute_reply.started":"2021-10-23T18:02:21.030964Z","shell.execute_reply":"2021-10-23T18:02:21.040695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 style=\"text-align:center;\">EDA</h1>","metadata":{"execution":{"iopub.execute_input":"2021-10-22T01:19:45.307607Z","iopub.status.busy":"2021-10-22T01:19:45.307331Z","iopub.status.idle":"2021-10-22T01:19:45.42632Z","shell.execute_reply":"2021-10-22T01:19:45.425474Z","shell.execute_reply.started":"2021-10-22T01:19:45.307579Z"}}},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">1. Exploring dependent variables</h3><br>\n<h4>Outline</h4>\nThe dependent variable is a continuous variable. The goal is to solve the regression analysis problem.<br>\nIn order to improve the performance of the linear models, the dependent variables and residuals must satisfy <b>'normality'</b>.<br><br>\n\n<h4>Normality</h4>\nNormality means that the distribution of variables follows a normal distribution.<br>\nThe easiest way to test normality is to draw a histogram and a QQ plot.<br>\nIt is recommended that the histogram form a bell shape. It is good to understand to see skewness and kurtosis together.<br>\nI thought that if skewness is between 0 and 0.5 and kurtosis is between 1 and 8, the shape of the histogram is similar to that of a bell.<br>\nIf there is a shape extending along the baseline to the top right in the QQ plot, we can see that the data has normality.<br><br>\nIf it violates normality, log transformation or boxcox transformation can be applied.","metadata":{"execution":{"iopub.execute_input":"2021-10-22T01:14:56.157801Z","iopub.status.busy":"2021-10-22T01:14:56.15721Z","iopub.status.idle":"2021-10-22T01:14:56.189651Z","shell.execute_reply":"2021-10-22T01:14:56.18883Z","shell.execute_reply.started":"2021-10-22T01:14:56.15764Z"}}},{"cell_type":"code","source":"print(f\"SalePrice's skew: {train.SalePrice.skew()}\")\nprint(f\"SalePrice's kurt: {train.SalePrice.kurt()}\")\nf, ax = plt.subplots(1, 2, figsize=(20, 5))\nsns.distplot(train.SalePrice, fit=norm, ax=ax[0])\nprobplot(train.SalePrice, plot=ax[1])\nplt.show()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2021-10-23T18:02:21.043152Z","iopub.execute_input":"2021-10-23T18:02:21.043439Z","iopub.status.idle":"2021-10-23T18:02:21.801626Z","shell.execute_reply.started":"2021-10-23T18:02:21.043411Z","shell.execute_reply":"2021-10-23T18:02:21.800777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n    <font color='red'>Conclusion:</font><br>\n    Because the histogram is skewed to the left (skewness > 0), and the points of the QQ plot are out of the baseline,<br>\n    It is necessary to convert to satisfy normality through variable transformation such as log transformation.\n</p>","metadata":{}},{"cell_type":"markdown","source":"Check whether there is a missing value of the dependent variable.","metadata":{}},{"cell_type":"code","source":"print(f\"SalePrice'missing count: {train.SalePrice.isnull().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:21.803054Z","iopub.execute_input":"2021-10-23T18:02:21.803315Z","iopub.status.idle":"2021-10-23T18:02:21.808492Z","shell.execute_reply.started":"2021-10-23T18:02:21.803286Z","shell.execute_reply":"2021-10-23T18:02:21.807924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">2. Exploring independent variables - Define Type</h3><br>\n<h4>Outline</h4>\n<p>\n    Variable search means reading data_description and determining type of variables.<br>\n    We can read the description of the variable, determine the type of variable, and further derive ideas for generating derived variables and converting data.<br><br>\n    The types of variables are classified as follows.<br>\n    <ol>\n        <li>Categorical</li>\n        <ol>\n            <li>Nominal vars</li>\n            <li>Order(Rank) vars</li>\n        </ol>\n        <li>Continuous</li>\n        <ol>\n            <li>Interval vars</li>\n            <li>Ratio vars</li>\n        </ol>\n    </ol>\n    The method of searching and preprocessing is determined by the type of variable.<br>\n    <br>\n    By using Pandas, we can find out the data type (numerical type and object) of the variable.<br>\n    However, not all numerical variables can be determined as continuous variables.<br>\n    I divided them subjectively by referring to data_description.<br>\n    <br>\n    Criteria (subjective)\n    <ol>\n        <li>\n            Nominal type: Not related by category or item.<br>\n            ex) Country, type\n        </li>\n        <li>\n            Rank (order) type: Rank, order, and comparison are possible for each category and item.<br>\n            ex) Grade, quality, etc.\n        </li>\n        <li>\n            Continuous: A numerical variable that can be expressed continuously in a real interval.<br>\n            ex) Feet, number, etc.\n        </li>\n    </ol>\n</p>\nCheck numerical and object variables.","metadata":{}},{"cell_type":"code","source":"num_vars = train.columns[train.dtypes != 'object']\nobj_vars = train.columns[train.dtypes == 'object']","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:21.809532Z","iopub.execute_input":"2021-10-23T18:02:21.809729Z","iopub.status.idle":"2021-10-23T18:02:21.82088Z","shell.execute_reply.started":"2021-10-23T18:02:21.809704Z","shell.execute_reply":"2021-10-23T18:02:21.820211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(num_vars, columns=['num vars']).T","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:21.822094Z","iopub.execute_input":"2021-10-23T18:02:21.823138Z","iopub.status.idle":"2021-10-23T18:02:21.863378Z","shell.execute_reply.started":"2021-10-23T18:02:21.8231Z","shell.execute_reply":"2021-10-23T18:02:21.862329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(obj_vars, columns=['obj vars']).T","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:21.864719Z","iopub.execute_input":"2021-10-23T18:02:21.86501Z","iopub.status.idle":"2021-10-23T18:02:21.905849Z","shell.execute_reply.started":"2021-10-23T18:02:21.86498Z","shell.execute_reply":"2021-10-23T18:02:21.905026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Explore some variables for example.<br><br>\n<b>MSSubClass</b><br>\nMSSubClass is a variable representing the type of residence.<br>\nAlthough it is numerical, it is actually a categorical variable.<br>\nI was able to read the description of each category in detail and find keywords representing 'order' like keywords such as 'Older' and 'Newer'.<br>\nTherefore, MSSubClass was defined as an ordered variable.<br>\n<br>\n+Going further<br>\nLooking at the description of MSSubClass, it can be seen that the category contains the following meanings.\n1) Residential style.<br>\n2) Years.<br>\n3) The number of floors.<br><br>\nThe above items are all included in YearBuilt, HouseStyle, and BldgType, so they overlap.<br>\nInstead, several categories have an additional meaning of 'PUD'.<br>\nTherefore, it is possible to attempt to add a nominal derivative variable with a binary value based on PUD in the future.<br>\nIn addition, 1945 and 1946 often appear in explanations.<br>\nSince this period may have an important domain meaning, it can be used when exploring YeaBuilt in the future.\n<br><br>\n<b>MSZoning</b><br>\nMSZoning has a category. Each item is defined as a nominal variable because it does not have an order or ranking with each other.<br><br>\n<b>LotFrontage</b><br>\nLotFrontage is a continuous variable because it is a feet value.\n<br><br>\n<b>Street and Alley</b><br>\nStreet and Ally are nominal variables because there is no order and ranking.<br>\nHowever, except for the NA value, They have Grvl or Pave. These variables are okay to be processed with ordered variables (applying label encoding instead of one hot encoding).<br><br>\n+Going further<br>\nAlly may have a negative meaning (NA) value.<br>\nIn the previous study, I learned that negative values can be recorded as missing values.<br>\nTherefore, if there is a missing value, it can be directly replaced with a value such as None.\n<br><br>\n<b>OverallQual</b><br>\nOverallQual is numerical but have categories.<br>\nIt can be seen that each item has a meaning related to quality. Therefore, it is a ranking variable.<br>\nRanked variables apply label encoding to preserve the meaning of values. However, there is no need to separately encode ranked variables that have already been expressed in sequential figures, such as 1 to 10.<br>\nSo I just included it in the continuous variable.\n<br><br>\n<b>ExterQual and ExterCond</b><br>\nExterQual and ExterCond are categories.\n<br><br>\n<b>BsmtFullBath and BsmtHalfBath</b><br>\nIn fact, the two variables commonly refer to the number of Bathrooms.<br>\nI can think of creating a dataset with the same meaning as fewer variables by combining variables with one variable (ex) BsmtFullBath + 0.5 * BsmtHalfBath).\n<br><br>\n<b>Fence</b><br>\nFence is categorical and can be defined as a ranking variable because the item has a quality-related meaning.<br>\nHowever, it is divided into a private case and a Wood case.<br>\nTherefore, I can think that I can divide it into Privacy and Wood.\n<br><br>\n<b>MoSold and YrSold</b><br>\nI treated variables such as YearBuild and YearEmodAdd as continuous variables.<br>\nBut when I checked the number of values in MoSold and YrSold (I used value_counts), the number of values were small enough to be treated as order variables.<br>\nTherefore, the two variables were defined as order variables.<br><br>\nI conducted the above search for all variables (if there is anything wrong or better idea, please comment).<br><br>\nNow we can define the types of all variables.","metadata":{}},{"cell_type":"code","source":"nominal_vars = [\n    'MSZoning', 'LandContour', 'Utilities',\n    'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n    'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', \n    'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'Electrical', \n    'GarageType', 'MiscFeature', \n    'SaleType', 'SaleCondition'\n]","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:21.907496Z","iopub.execute_input":"2021-10-23T18:02:21.908809Z","iopub.status.idle":"2021-10-23T18:02:21.923905Z","shell.execute_reply.started":"2021-10-23T18:02:21.908751Z","shell.execute_reply":"2021-10-23T18:02:21.922725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"order_vars = [\n    'OverallCond', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual',\n    'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'Street', 'Alley',\n    'LandSlope', 'Functional', 'GarageFinish', 'MoSold', 'YrSold', 'PavedDrive', \n    'CentralAir', 'LotShape', 'MSSubClass', \n]","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:21.925854Z","iopub.execute_input":"2021-10-23T18:02:21.926132Z","iopub.status.idle":"2021-10-23T18:02:21.935077Z","shell.execute_reply.started":"2021-10-23T18:02:21.926103Z","shell.execute_reply":"2021-10-23T18:02:21.934135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"continuous_vars = [\n    'LotFrontage', 'LotArea', 'OverallQual', 'YearBuilt', 'YearRemodAdd', \n    'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n    '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', \n    'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt',\n    'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', \n    '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'\n]","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:26.447593Z","iopub.execute_input":"2021-10-23T18:02:26.4485Z","iopub.status.idle":"2021-10-23T18:02:26.453955Z","shell.execute_reply.started":"2021-10-23T18:02:26.448448Z","shell.execute_reply":"2021-10-23T18:02:26.453326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"+After determining the types of all variables, I checked if there were any missing variables.<br><br>\nI compared the number of columns in the training set with the sum of self-defined variables and found that the number was different(Except Id, SalePrice).<br>\nI found in the data_description that the variables expressed in Bedroom and Kitchen are actually Bedroom AbvGr and Kitchen AbvGr. These were defined as continuous variables.","metadata":{}},{"cell_type":"code","source":"len(train.columns) == (len(nominal_vars) + len(order_vars)\n                       + len(continuous_vars)) + len(set(['Id', 'SalePrice']))","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:33.008354Z","iopub.execute_input":"2021-10-23T18:02:33.0096Z","iopub.status.idle":"2021-10-23T18:02:33.0165Z","shell.execute_reply.started":"2021-10-23T18:02:33.009535Z","shell.execute_reply":"2021-10-23T18:02:33.015851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n    <font color='red'>Conclusion:</font><br>\n    We defined the types of all variables.<br>\n    This process can be effective in the pretreatment part later.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">3. Exploring independent variables - Check the missing values</h3><br>\n<h4>Outline</h4>\nWe need to make sure that missing values exist for all variables.<br>\nisnull().Sum() can be used to find missing values.","metadata":{}},{"cell_type":"code","source":"all_data = pd.concat((train, test)).drop(['SalePrice'], axis=1)\ncnt_missing = all_data.isnull().sum().sort_values(ascending=False)\ncnt_percent = cnt_missing / all_data.shape[0]\nmissing_table = pd.DataFrame([cnt_missing, cnt_percent], \n                             index=['missing count', 'missing percent']).T\nmissing_table = missing_table[missing_table['missing count'] > 0]","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:38.118596Z","iopub.execute_input":"2021-10-23T18:02:38.11959Z","iopub.status.idle":"2021-10-23T18:02:38.164294Z","shell.execute_reply.started":"2021-10-23T18:02:38.119533Z","shell.execute_reply":"2021-10-23T18:02:38.163436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_table.iloc[:20]","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:15:59.77964Z","iopub.execute_input":"2021-10-23T18:15:59.779967Z","iopub.status.idle":"2021-10-23T18:15:59.794957Z","shell.execute_reply.started":"2021-10-23T18:15:59.779934Z","shell.execute_reply":"2021-10-23T18:15:59.79384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(20, 8))\nsns.barplot(x=missing_table.index, y=missing_table['missing percent'])\nplt.title('Missing Percent by Values', size=20)\nplt.xlabel('Values', size=15)\nplt.ylabel('Percent', size=15)\nplt.xticks(rotation='90')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:02:38.821708Z","iopub.execute_input":"2021-10-23T18:02:38.822655Z","iopub.status.idle":"2021-10-23T18:02:39.463659Z","shell.execute_reply.started":"2021-10-23T18:02:38.82261Z","shell.execute_reply":"2021-10-23T18:02:39.46254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n    <font color='red'>Conclusion:</font><br>\n    Many variables have missing values.<br>\n    Some variables have extremely large amounts of missing values.<br>\n    We previously found category variables with negative meanings.<br>\n    Maybe we can handle them easily.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">4. Exploring independent variables - distribution</h3><br>\nI used histogram and barchat to see the distribution of continuous and categorical variables.","metadata":{}},{"cell_type":"markdown","source":"Draw a histogram of continuous variables.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(6, 6, figsize=(25, 20))\nfor i, c in enumerate(continuous_vars):\n    sns.distplot(all_data[c], fit=norm, ax=ax[i//6, i%6])","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:09:10.692144Z","iopub.execute_input":"2021-10-23T18:09:10.692725Z","iopub.status.idle":"2021-10-23T18:09:22.471297Z","shell.execute_reply.started":"2021-10-23T18:09:10.692689Z","shell.execute_reply":"2021-10-23T18:09:22.470398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some variables seem to be able to follow a normal distribution through log transformation or box cox transformation. Some variables have zero. It seems that +1 should be done when converting.\n\nDraw a bar chart for categorical variables.\n\nSome variables were extremely biased toward one value (0). So they don't seem to be very important variables.","metadata":{}},{"cell_type":"code","source":"f, ax = plt.subplots(5, 5, figsize=(25, 15))\nfor i, c in enumerate(nominal_vars):\n    g = sns.barplot(data=pd.DataFrame(all_data[c].value_counts()).reset_index(), x='index', y=c, ax=ax[i//5, i%5])\n    g.set(xticks=[])\n    g.set(title=c)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:09:53.021315Z","iopub.execute_input":"2021-10-23T18:09:53.021864Z","iopub.status.idle":"2021-10-23T18:09:57.482971Z","shell.execute_reply.started":"2021-10-23T18:09:53.021827Z","shell.execute_reply":"2021-10-23T18:09:57.481987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">5. Bivariate search - correlation analysis, heat map, finding important variables</h3><br>\n<h4>Outline</h4>\nCorrelation analysis is a technique to find out the correlation between the two variables.<br>\nWe can find variables with relatively strong <b>'linearity'</b> using the correlation coefficient with the dependent variable.<br>\nI designated these variables as relatively important variables and tested <b>'homogeneity'</b>.<br>\nIn addition, I tested <b>'independence'</b> through the correlation between independent variables.<br>\n<br>\n<h4>Some assumptions for a good regression model</h4>\nPreviously, we checked the normality of the dependent variable and the residual.<br>\nIn addition, several more assumptions are needed.<br>\n<ol>\n    <li>\n        <b>Linearity</b>:<br>\n        It is recommended that the independent variable and the dependent variable have a linear relationship.<br>\n        The 'variable transformation' or 'dimensional increase' method can help to have linearity.\n    </li>\n    <li>\n        <b>homogeneity:</b>:<br>\n        The variance of the residuals must be constant.<br>\n        Drawing a residual diagram for an independent variable can test the equal variance.<br>\n        If the points follow randomly based on the baseline, they satisfy the equal variance.\n    </li>\n    <li>\n        <b>Independence</b>:<br>\n        Independence means that there should be no correlation between independent variables.<br>\n        The high correlation between independent variables causes multicollinearity. As a result, the performance of the model becomes incredible.\n    </li>\n    <li>\n        <b>Irregularity</b>:<br>\n        There should be no correlation between residuals.<br>\n        Durbin-Watson' helps test for non-correlation.\n    </li>\n</ol>","metadata":{"execution":{"iopub.execute_input":"2021-10-19T04:10:46.609454Z","iopub.status.busy":"2021-10-19T04:10:46.609145Z","iopub.status.idle":"2021-10-19T04:10:46.639747Z","shell.execute_reply":"2021-10-19T04:10:46.638655Z","shell.execute_reply.started":"2021-10-19T04:10:46.609425Z"}}},{"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 7))\nhighcorr_vars = (abs(train.corr().SalePrice).sort_values(ascending=False)[:7]).index\nsns.heatmap(train[highcorr_vars].corr(), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:03:11.265376Z","iopub.execute_input":"2021-10-23T18:03:11.2665Z","iopub.status.idle":"2021-10-23T18:03:11.831444Z","shell.execute_reply.started":"2021-10-23T18:03:11.266431Z","shell.execute_reply":"2021-10-23T18:03:11.830561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I selected only the top 10 variables with the highest correlation with the dependent variable and conducted the correlation analysis again.\n\n\"OverallQual\" and \"GrLiv Area\" are the strongest variables.\n\nGarage Cars and Garage Area are also highly correlated. High correlation between independent variables is not good because it causes multicollinearity.","metadata":{}},{"cell_type":"code","source":"def hypo_test(x, y, cat=False):\n    f, ax = plt.subplots(1, 4, figsize=(25, 5))\n    if cat:\n        sns.boxplot(x=train[x], y=train[y], ax=ax[0])\n    else:\n        sns.scatterplot(x=train[x], y=train[y], ax=ax[0])\n        sns.regplot(x=train[x], y=train[y], ax=ax[0])\n    sns.residplot(x=train[x], y=train[y], ax=ax[1])\n    sns.distplot(train[x], fit=norm, ax=ax[2])\n    probplot(train[x], plot=ax[3])\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:03:13.67935Z","iopub.execute_input":"2021-10-23T18:03:13.680195Z","iopub.status.idle":"2021-10-23T18:03:13.690106Z","shell.execute_reply.started":"2021-10-23T18:03:13.680155Z","shell.execute_reply":"2021-10-23T18:03:13.689261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I used the following chart for some tests.<br>\nScatter, Boxplot, resid plot, histogram, QQplot","metadata":{}},{"cell_type":"code","source":"hypo_test('OverallQual', 'SalePrice', True)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:03:15.839016Z","iopub.execute_input":"2021-10-23T18:03:15.839437Z","iopub.status.idle":"2021-10-23T18:03:17.028742Z","shell.execute_reply.started":"2021-10-23T18:03:15.839408Z","shell.execute_reply":"2021-10-23T18:03:17.02748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hypo_test('GrLivArea', 'SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:03:18.509418Z","iopub.execute_input":"2021-10-23T18:03:18.509732Z","iopub.status.idle":"2021-10-23T18:03:19.70309Z","shell.execute_reply.started":"2021-10-23T18:03:18.509702Z","shell.execute_reply":"2021-10-23T18:03:19.702078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hypo_test('GarageArea', 'SalePrice')","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:03:19.70474Z","iopub.execute_input":"2021-10-23T18:03:19.705102Z","iopub.status.idle":"2021-10-23T18:03:20.893555Z","shell.execute_reply.started":"2021-10-23T18:03:19.705068Z","shell.execute_reply":"2021-10-23T18:03:20.8927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, ax = plt.subplots(1, 2, figsize=(12, 5))\nsns.scatterplot(y=train.GarageArea, x=train.GarageCars, ax=ax[0])\nsns.scatterplot(x=train['1stFlrSF'], y=train.TotalBsmtSF, ax=ax[1])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:03:25.748308Z","iopub.execute_input":"2021-10-23T18:03:25.748744Z","iopub.status.idle":"2021-10-23T18:03:26.471114Z","shell.execute_reply.started":"2021-10-23T18:03:25.748713Z","shell.execute_reply":"2021-10-23T18:03:26.470226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skews = abs(all_data.skew()).sort_values(ascending=False)\nkurts = abs(all_data.kurt()).sort_values(ascending=False)\nskew_kurt_table = pd.DataFrame([skews, kurts], index=['skew', 'kurt']).T\nntv = skew_kurt_table[skew_kurt_table['skew'] > 0.5].index\n\nplt.subplots(figsize=(15, 5))\nsns.barplot(x=skew_kurt_table.loc[ntv].index, y=skew_kurt_table.loc[ntv]['skew'])\nplt.xticks(rotation='90')\nplt.title('skew by variable', size=20)\nplt.xlabel('vars', size=15)\nplt.ylabel('skew', size=15)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:03:28.73557Z","iopub.execute_input":"2021-10-23T18:03:28.73592Z","iopub.status.idle":"2021-10-23T18:03:29.288633Z","shell.execute_reply.started":"2021-10-23T18:03:28.735866Z","shell.execute_reply":"2021-10-23T18:03:29.28766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n    <font color='red'>Conclusion:</font><br>\n    I analyzed the correlation of each variable for SalePrice and confirmed that variables such as OverallQual, GrLivArea, and CarArea had high linearity.<br>\n    Linearity was visualized using a box plot and scatter plot.<br>\n    And I thought these variables were important variables, so I drew a residual diagram for the homodis variance test, a histogram for the normality test, and a QQ plot for them.<br>\n    <br>\n    Each variable does not satisfy the equal variance because the points of the residual degree are not randomly sprayed and have some pattern or shape.<br>\n    I could see that each variable had no ideal normality through the results of the histogram and QQplot.<br>\n    The above problems may be solved through log transformation or boxcox transformation.<br>\n    <br>\n    It was also confirmed that some independent variables were correlated with each other. As expected, I visualized it as a scatter plot.<br>\n    The high correlation between independent variables causes multicollinearity. The explanatory power of the model loses its reliability.<br>\n    I decided to use regulation rather than choosing variables or using dimension reduction right away.<br>\n    The linear model may solve the above problem through regulation (normal1, normal2).<br>\n    <br>\n    It was difficult to visualize all variables, so I looked at the independent variables that required conversion through skewness and kurtosis.\n</p>","metadata":{}},{"cell_type":"markdown","source":"Draw pair plot!","metadata":{}},{"cell_type":"code","source":"sns.pairplot(train[highcorr_vars])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:13:34.478662Z","iopub.execute_input":"2021-10-23T18:13:34.479167Z","iopub.status.idle":"2021-10-23T18:13:44.777022Z","shell.execute_reply.started":"2021-10-23T18:13:34.479132Z","shell.execute_reply":"2021-10-23T18:13:44.775862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style='text-align: center'>Preprocessing</h2>","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">1. Remove ID</h3><br>\n<h4>Outline</h4>\nI removed the ID variable that was not needed for analysis.","metadata":{}},{"cell_type":"code","source":"train_id = train.Id\ntest_id = test.Id\ntrain.drop(['Id'], axis=1, inplace=True)\ntest.drop(['Id'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:15:26.168958Z","iopub.execute_input":"2021-10-23T18:15:26.16931Z","iopub.status.idle":"2021-10-23T18:15:26.180633Z","shell.execute_reply.started":"2021-10-23T18:15:26.16927Z","shell.execute_reply":"2021-10-23T18:15:26.179435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">2. Clensing - Outlier</h3><br>\n<h4>Outline</h4>\nA dataset with outliers can degrade the performance of the model.<br>\nIt is optional to remove outliers existing in the training data. I found two outliers in the scatter plot of the variable with a strong correlation with the dependent variable.<br>\nThe two points were located in a place far off the straight line.<br><br>\n\n<h4>Finding outliers</h4>\n<ol>\n    <li><b>Statistics for univariate - normal distribution, IQR:</b><br>For independent variables, points outside the threshold can be judged as outliers using normal distributions, likelihood functions, IQR, etc.<br>\n        However, it cannot be judged that the point outside the threshold is always an outlier. For example, the scatterplot of GrLiv Area has two points apart at the top right.<br>\n        These differ greatly in value from other points, but they are important because they can prove linearity.<br>Therefore, it is necessary to carefully deal with the determination of outliers for univariate quantities.</li>\n    <li><b>Scattering point:</b><br>If you draw a scatterplot of two variables with a pattern (e.g., a linear relationship), you can intuitively find outliers.<br>\n        I previously found some variables that have a strong correlation with the dependent variable. Two outliers were found through GrLiv Area's scatter plot.</li>\n    <li><b>Clustering - DBSCAN:</b><br>DBSCAN can detect outliers using distance.<br>\n        DBSCAN has a set range (epsilon) and required peripheral points (min_samples), and generates clusters by calculating key points and peripheral points.<br>\n        Points that do not have key points around and do not have the minimum required neighboring points are outliers.<br>\n        I applied scaling to GrLiv Area and SalePrice and tried DBSCAN.<br></li>\n</ol>","metadata":{}},{"cell_type":"code","source":"plt.subplots(figsize=(20, 10))\noutlier_idx = train.GrLivArea.sort_values(ascending=False)[:2].index\nsns.scatterplot(x=train['GrLivArea'], y=train.SalePrice)\nsns.scatterplot(x=train.iloc[outlier_idx]['GrLivArea'], y=train.iloc[outlier_idx].SalePrice, color='r', s=300, alpha=.6)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:15:28.920426Z","iopub.execute_input":"2021-10-23T18:15:28.9207Z","iopub.status.idle":"2021-10-23T18:15:29.647768Z","shell.execute_reply.started":"2021-10-23T18:15:28.920672Z","shell.execute_reply":"2021-10-23T18:15:29.647113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled_data = pd.DataFrame(StandardScaler().fit_transform(train[['GrLivArea', 'SalePrice']]), columns=['GrLivArea', 'SalePrice'])\ndbscan_model = DBSCAN(eps=1.5, min_samples=3).fit(scaled_data)\ntmp = pd.concat((scaled_data, pd.DataFrame(dbscan_model.labels_, columns=['label'])), axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:17.257712Z","iopub.execute_input":"2021-10-23T18:16:17.258525Z","iopub.status.idle":"2021-10-23T18:16:17.30933Z","shell.execute_reply.started":"2021-10-23T18:16:17.258485Z","shell.execute_reply":"2021-10-23T18:16:17.308376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:18.255387Z","iopub.execute_input":"2021-10-23T18:16:18.25566Z","iopub.status.idle":"2021-10-23T18:16:18.264362Z","shell.execute_reply.started":"2021-10-23T18:16:18.255633Z","shell.execute_reply":"2021-10-23T18:16:18.263396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(tmp, hue='label', size=5)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:19.135746Z","iopub.execute_input":"2021-10-23T18:16:19.136089Z","iopub.status.idle":"2021-10-23T18:16:21.109444Z","shell.execute_reply.started":"2021-10-23T18:16:19.136057Z","shell.execute_reply":"2021-10-23T18:16:21.108277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.drop(train.GrLivArea.sort_values(ascending=False)[:2].index, axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:21.718289Z","iopub.execute_input":"2021-10-23T18:16:21.718634Z","iopub.status.idle":"2021-10-23T18:16:21.727478Z","shell.execute_reply.started":"2021-10-23T18:16:21.718602Z","shell.execute_reply":"2021-10-23T18:16:21.726379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n    <font color='red'>Conclusion:</font><br>\n    Two outliers were found through the scatterplot. Two points were removed.<br>\n    In the case of DBSCAN, a total of 4 points were judged as outliers.\n</p>","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">3. Train, Test merge, separating dependent variables.</h3><br>\n<h4>Outline</h4>\nI removed outliers from the training data. Now I combine with the test data. The dependent variable is separated separately.","metadata":{}},{"cell_type":"code","source":"train_size = train.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test), axis=0).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:25.307355Z","iopub.execute_input":"2021-10-23T18:16:25.307651Z","iopub.status.idle":"2021-10-23T18:16:25.336649Z","shell.execute_reply.started":"2021-10-23T18:16:25.307622Z","shell.execute_reply":"2021-10-23T18:16:25.335964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape, test.shape, all_data.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:26.464981Z","iopub.execute_input":"2021-10-23T18:16:26.465657Z","iopub.status.idle":"2021-10-23T18:16:26.473554Z","shell.execute_reply.started":"2021-10-23T18:16:26.465613Z","shell.execute_reply":"2021-10-23T18:16:26.472451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">4. Clensing - Missing Value</h3><br>\n<h4>Outline</h4>\nMissing values are objects that must be removed. The missing values are processed based on the understanding of each independent variable.\n<br><br>\n<h4>How to deal with missing values.</h4>\n<ol>\n    <li><b>Delete:</b><br>\n        It is the easiest and most powerful way. Clear rows or columns. However, avoid erasing rows because missing values may also exist in the test data.<br>\n        Clearing the columns risks removing important variables, so avoid them if possible.</li>\n    <li><b>Replace a specific value:</b><br>\n        It is a way to try if you have knowledge of variables. For example, if there are extremely many missing values of iso-interstitial variables that can have negative meanings,<br>\n        Missing values are likely to have a negative meaning.\n    <li><b>Replacement of central propensity:</b><br>\n        It can be replaced with a central tendency value such as an average, a median value, and a minimum value. It is possible to replace the continuous variable with the median value and the category variable with the lowest value.</li>\n    <li><b>Other than that: Simple probability replacement, multiple confrontation method, etc.</b><br>\n</ol>\nAfter looking at the explanation of each variable, I treated it as follows.<br>\n<ul>\n    <li>\n        Continuous: Select one relevant category variable and replace the median for each category.<br>or\n        Alternate to 0 if it is bound to be a missing value.\n    </li>\n    <li>\n        Category type: value_counts to identify the distribution and replace it with None if there is zero negative meaning, such as NA or POOL. or<br>Select one related category variable, identify the distribution of values for each category, and replace the poorest value.\n    </li>\n</ul>","metadata":{}},{"cell_type":"markdown","source":"<p>\n    Variables that are currently missing.\n</p>","metadata":{}},{"cell_type":"code","source":"all_data.columns[all_data.isnull().sum() > 0]","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:30.743103Z","iopub.execute_input":"2021-10-23T18:16:30.743529Z","iopub.status.idle":"2021-10-23T18:16:30.766745Z","shell.execute_reply.started":"2021-10-23T18:16:30.743499Z","shell.execute_reply":"2021-10-23T18:16:30.765613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ex1) LotFrontage is replaced by the median value per Neighborhood.","metadata":{}},{"cell_type":"code","source":"sns.scatterplot(x=all_data.Neighborhood, y=all_data.LotFrontage)\nplt.xticks(rotation='90')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:32.825893Z","iopub.execute_input":"2021-10-23T18:16:32.826181Z","iopub.status.idle":"2021-10-23T18:16:33.245578Z","shell.execute_reply.started":"2021-10-23T18:16:32.826154Z","shell.execute_reply":"2021-10-23T18:16:33.244699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ex2) Ally is a ranking variable that can have NA, and since these values have been treated as missing values, replace them with None.","metadata":{}},{"cell_type":"code","source":"all_data.Alley.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:35.609956Z","iopub.execute_input":"2021-10-23T18:16:35.610402Z","iopub.status.idle":"2021-10-23T18:16:35.619394Z","shell.execute_reply.started":"2021-10-23T18:16:35.610357Z","shell.execute_reply":"2021-10-23T18:16:35.618229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ex3) Utilities are extremely skewed to AllPub, so replace them with the lowest value.","metadata":{}},{"cell_type":"code","source":"all_data.Utilities.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:36.974358Z","iopub.execute_input":"2021-10-23T18:16:36.974649Z","iopub.status.idle":"2021-10-23T18:16:36.982872Z","shell.execute_reply.started":"2021-10-23T18:16:36.974621Z","shell.execute_reply":"2021-10-23T18:16:36.982217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ex4) Garage Area is a missing value in the absence of Garage, so it is replaced by 0.","metadata":{}},{"cell_type":"code","source":"all_data.GarageArea.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:37.990382Z","iopub.execute_input":"2021-10-23T18:16:37.990841Z","iopub.status.idle":"2021-10-23T18:16:38.002386Z","shell.execute_reply.started":"2021-10-23T18:16:37.990807Z","shell.execute_reply":"2021-10-23T18:16:38.001308Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all_data.MSZoning = all_data.groupby('Neighborhood').MSZoning.transform(lambda x: x.fillna(x.mode()[0]))\nall_data.LotFrontage = all_data.groupby('Neighborhood').LotFrontage.transform(lambda x: x.fillna(x.median()))\n#all_data.Exterior1st = all_data.groupby('Neighborhood').Exterior1st.transform(lambda x: x.fillna(x.mode()[0]))\n#all_data.Exterior2nd = all_data.groupby('Neighborhood').Exterior2nd.transform(lambda x: x.fillna(x.mode()[0]))\n\nfor c in ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n          'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual',\n          'GarageCond', 'PoolQC', 'Fence', 'MiscFeature']:\n    all_data[c] = all_data[c].fillna('None')\n    \nfor c in ['MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n         'BsmtHalfBath', 'GarageYrBlt', 'GarageCars', 'GarageArea']:\n    all_data[c] = all_data[c].fillna(0)   \n    \nfor c in ['MSZoning', 'Exterior1st', 'Exterior2nd', 'Electrical', 'Utilities', 'SaleType', 'Functional', 'KitchenQual']:\n    all_data[c] = all_data[c].fillna(all_data[c].mode()[0])","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:56.493031Z","iopub.execute_input":"2021-10-23T18:16:56.49349Z","iopub.status.idle":"2021-10-23T18:16:56.545734Z","shell.execute_reply.started":"2021-10-23T18:16:56.493458Z","shell.execute_reply":"2021-10-23T18:16:56.544644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data.isnull().sum().sum()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:57.235491Z","iopub.execute_input":"2021-10-23T18:16:57.235803Z","iopub.status.idle":"2021-10-23T18:16:57.259312Z","shell.execute_reply.started":"2021-10-23T18:16:57.235773Z","shell.execute_reply":"2021-10-23T18:16:57.258281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">5. Derivative variable generation</h3><br>\n\nDerivative variables are methods that can improve the quality of analysis. There are several ideas for how to generate derivative variables.\n\nIt is a continuous variable and means an observation of an object, and if this value is 0, it means that there is no object.\nTherefore, it is possible to add categorical variables as to whether or not the object is present. Categorical variables with binary values can be stored as 0 and 1.\n\nIf you can express continuous variables of the same series in association, use a four-line operation.\nNew variables can be created. It is similar to calculating BMI with height and weight.\n\nThe above judgment can be used using min and max values after describe.\n\n","metadata":{}},{"cell_type":"code","source":"all_data['HasMasVnr'] = all_data.MasVnrArea.apply(lambda x: 1 if x else 0)\nall_data['Has2ndFlrSF'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x else 0)\nall_data['HasGarageArea'] = all_data['GarageArea'].apply(lambda x: 1 if x else 0)\nall_data['HasWoodDeckSF'] = all_data['WoodDeckSF'].apply(lambda x: 1 if x else 0)\nall_data['HasOpenPorchSF'] = all_data['OpenPorchSF'].apply(lambda x: 1 if x else 0)\nall_data['HasEnclosedPorch'] = all_data['EnclosedPorch'].apply(lambda x: 1 if x else 0)\nall_data['Has3SsnPorch'] = all_data['3SsnPorch'].apply(lambda x: 1 if x else 0)\nall_data['HasScreenPorch'] = all_data['ScreenPorch'].apply(lambda x: 1 if x else 0)\nall_data['HasPoolArea'] = all_data['PoolArea'].apply(lambda x: 1 if x else 0)\nall_data['HasMiscVal'] = all_data['MiscVal'].apply(lambda x: 1 if x else 0)\n\nall_data['TotalBath'] = all_data['BsmtFullBath'] + all_data['BsmtHalfBath'] * 0.5\\\n+ all_data['FullBath'] + all_data['HalfBath'] * 0.5\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'] + all_data['BsmtFinSF1'] \\\n+ all_data['BsmtFinSF2']","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:16:59.418824Z","iopub.execute_input":"2021-10-23T18:16:59.419146Z","iopub.status.idle":"2021-10-23T18:16:59.464554Z","shell.execute_reply.started":"2021-10-23T18:16:59.419117Z","shell.execute_reply":"2021-10-23T18:16:59.463622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">6. Select variables</h3><br>\nUsing variable selection can help improve the performance of the model.<br>\nThere are various approaches to the variable selection method, but I removed only a few variables that were most simply biased to one side.<br>","metadata":{}},{"cell_type":"code","source":"nominal_vars = list(set(nominal_vars) - set(['Utilities']))\n#order_vars = list(set(order_vars) - set(['Utilities']))\nall_data.drop(['Utilities'], axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:17:49.257841Z","iopub.execute_input":"2021-10-23T18:17:49.25819Z","iopub.status.idle":"2021-10-23T18:17:49.267695Z","shell.execute_reply.started":"2021-10-23T18:17:49.258157Z","shell.execute_reply":"2021-10-23T18:17:49.267028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# nominal_vars = list(set(nominal_vars) - set(['Utilities', 'PoolQC', 'PoolArea', 'Street']))\n# order_vars = list(set(order_vars) - set(['Utilities', 'PoolQC', 'PoolArea', 'Street']))\n# all_data.drop(['Utilities', 'PoolQC', 'PoolArea', 'Street'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">7. Transform variables</h3><br>\nLog and boxcox transformations are applied to secure the normality and equal variance of continuous variables.<br>\nLabel encoding is applied to ordered (priority) variables.<br>\nOne-hot encoding is applied to equal variables.<br>\nLog transformation is applied to the dependent variable to secure the normality of the dependent variable.","metadata":{}},{"cell_type":"code","source":"all_data[nominal_vars] = all_data[nominal_vars].astype(str)\nall_data[order_vars] = all_data[order_vars].astype(str)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:17:58.496834Z","iopub.execute_input":"2021-10-23T18:17:58.497815Z","iopub.status.idle":"2021-10-23T18:17:58.541995Z","shell.execute_reply.started":"2021-10-23T18:17:58.497766Z","shell.execute_reply":"2021-10-23T18:17:58.541129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for c in order_vars:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:17:59.273889Z","iopub.execute_input":"2021-10-23T18:17:59.27424Z","iopub.status.idle":"2021-10-23T18:17:59.366531Z","shell.execute_reply.started":"2021-10-23T18:17:59.274206Z","shell.execute_reply":"2021-10-23T18:17:59.365443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:17:59.934986Z","iopub.execute_input":"2021-10-23T18:17:59.935919Z","iopub.status.idle":"2021-10-23T18:17:59.973925Z","shell.execute_reply.started":"2021-10-23T18:17:59.935861Z","shell.execute_reply":"2021-10-23T18:17:59.972757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"skewness = skewness[abs(skewness) > 0.75]\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:02.016318Z","iopub.execute_input":"2021-10-23T18:18:02.017444Z","iopub.status.idle":"2021-10-23T18:18:02.077496Z","shell.execute_reply.started":"2021-10-23T18:18:02.017354Z","shell.execute_reply":"2021-10-23T18:18:02.076447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_data = pd.get_dummies(all_data)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:02.999007Z","iopub.execute_input":"2021-10-23T18:18:02.999331Z","iopub.status.idle":"2021-10-23T18:18:03.033716Z","shell.execute_reply.started":"2021-10-23T18:18:02.9993Z","shell.execute_reply":"2021-10-23T18:18:03.03276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = np.log1p(y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:05.099427Z","iopub.execute_input":"2021-10-23T18:18:05.099708Z","iopub.status.idle":"2021-10-23T18:18:05.105169Z","shell.execute_reply.started":"2021-10-23T18:18:05.099679Z","shell.execute_reply":"2021-10-23T18:18:05.104135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">7. Training, test data division</h3><br>\nDivide the training data and test data again.","metadata":{}},{"cell_type":"code","source":"X_train, X_test = all_data.iloc[:train_size, :], all_data.iloc[train_size:, :]","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:07.307139Z","iopub.execute_input":"2021-10-23T18:18:07.308101Z","iopub.status.idle":"2021-10-23T18:18:07.312751Z","shell.execute_reply.started":"2021-10-23T18:18:07.308056Z","shell.execute_reply":"2021-10-23T18:18:07.311984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_test.shape, y_train.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:08.165844Z","iopub.execute_input":"2021-10-23T18:18:08.166638Z","iopub.status.idle":"2021-10-23T18:18:08.173843Z","shell.execute_reply.started":"2021-10-23T18:18:08.166594Z","shell.execute_reply":"2021-10-23T18:18:08.172936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align:center;\">Optimization (GridSearch, Optuna)</h2><br>\nIn order to maximize the performance of the model, the process of finding the appropriate hyperparameters is required. I used GridSearch to find hyperparameters for each model.\n<br><br>\nSklearn's GridSearchCV was used for Laso, Ridge, ElasticNet, and SVM, and Optuna was used for tree-based models.","metadata":{}},{"cell_type":"code","source":"def rmsle_cv(model):\n    return np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring='neg_mean_squared_error',\n                   cv=5, verbose=0, n_jobs=-1))","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:18.169466Z","iopub.execute_input":"2021-10-23T18:18:18.169798Z","iopub.status.idle":"2021-10-23T18:18:18.175698Z","shell.execute_reply.started":"2021-10-23T18:18:18.169768Z","shell.execute_reply":"2021-10-23T18:18:18.174372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">1. GridSearch Cross Validation</h3><br>","metadata":{}},{"cell_type":"code","source":"model_lasso = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', Lasso())\n])\nmodel_elasticNet = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', ElasticNet(max_iter=5000))\n])\nmodel_krr = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', krr())\n])\n\nmodel_svr = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', SVR())\n])\n\ngrid_param_lasso = {\n    'model__alpha': 0.0001 * np.arange(1, 100)\n}\ngrid_param_elasticNet = {\n    'model__alpha': 0.0001 * np.arange(1, 100),\n    'model__l1_ratio': 0.001 * np.arange(1, 10)\n}\ngrid_param_krr = {\n    'model__alpha': 0.0001 * np.arange(1, 100),\n    'model__degree': [1, 2, 3],\n    'model__kernel': ['polynomial'],\n    'model__coef0': [2.5]\n}\ngrid_param_svr = {\n    'model__C': [0.001, 0.1, 1, 10, 20],\n    'model__gamma': [.0001, .0002, .0003, .0004, .0005, .0006, .0007, .0008, .0009, .001],\n    'model__epsilon': [.01, .02, .03, .04, .05, .06, .07, .08, .09, .1]\n}\n\nbest_params = {}","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:20.723971Z","iopub.execute_input":"2021-10-23T18:18:20.724255Z","iopub.status.idle":"2021-10-23T18:18:20.736629Z","shell.execute_reply.started":"2021-10-23T18:18:20.724227Z","shell.execute_reply":"2021-10-23T18:18:20.735545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_lasso = GridSearchCV(model_lasso, grid_param_lasso, scoring='neg_mean_squared_error',\n                           cv=5, n_jobs=-1, verbose=0).fit(X_train, y_train)\nbest_params['Lasso'] = search_lasso.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:22.764015Z","iopub.execute_input":"2021-10-23T18:18:22.764305Z","iopub.status.idle":"2021-10-23T18:18:48.091455Z","shell.execute_reply.started":"2021-10-23T18:18:22.764276Z","shell.execute_reply":"2021-10-23T18:18:48.090142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_elasticNet = GridSearchCV(model_elasticNet, grid_param_elasticNet, scoring='neg_mean_squared_error',\n                           cv=5, n_jobs=-1, verbose=0).fit(X_train, y_train)\nbest_params['ElasticNet'] = search_elasticNet.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:18:48.094752Z","iopub.execute_input":"2021-10-23T18:18:48.095577Z","iopub.status.idle":"2021-10-23T18:25:39.530988Z","shell.execute_reply.started":"2021-10-23T18:18:48.095522Z","shell.execute_reply":"2021-10-23T18:25:39.529733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_krr = GridSearchCV(model_krr, grid_param_krr, scoring='neg_mean_squared_error',\n                           cv=5, n_jobs=-1, verbose=0).fit(X_train, y_train)\nbest_params['KernelRidge'] = search_krr.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:25:39.53346Z","iopub.execute_input":"2021-10-23T18:25:39.534156Z","iopub.status.idle":"2021-10-23T18:27:36.975509Z","shell.execute_reply.started":"2021-10-23T18:25:39.534107Z","shell.execute_reply":"2021-10-23T18:27:36.974439Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"search_svr = GridSearchCV(model_svr, grid_param_svr, scoring='neg_mean_squared_error',\n                           cv=5, n_jobs=-1, verbose=0).fit(X_train, y_train)\nbest_params['SVR'] = search_svr.best_params_","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:27:36.979044Z","iopub.execute_input":"2021-10-23T18:27:36.979773Z","iopub.status.idle":"2021-10-23T18:34:39.005324Z","shell.execute_reply.started":"2021-10-23T18:27:36.979724Z","shell.execute_reply":"2021-10-23T18:34:39.004303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h3 style=\"text-align:center;\">2. Optuna</h3><br>\n\n<p>\nThe hyperparameters of tree models are diverse and have many combinations. Their optimization takes a lot of time.\n\nSo, I looked for XGBoost's hyperparameters using the Optuna package using early stopping and cross validation.\n\nThe hyperparameters of tree models are diverse and have many combinations. Their optimization takes a lot of time.\n\nI looked for XGBoost's hyperparameters using the Optuna package using early stopping and cross validation.\n\nPreparations: 'optuna', 'functions-partial', objective function\n\nPre-understanding:\n\nOptuna is a framework that helps optimize hyperparameters. An objective function is required.\n\nOptuna's objective function selects a new hyperparameter combination of the model every trial.\n\nOptuna's study object is an object that performs optimization. The optimization of the study object requires a partial object and the number of attempts.\n\nThe partial object is an object that binds X, y with the objective function to be used by optuna.\n\nStudy objects store results that meet the purpose for each trial. Finally, remember the most purposeful hyperparameter combination.\n\nThe trial factor of objective embeds the function of specifying the range and value of hyperparameters. It has a hyperparameter name, range or list as a factor in common.\n    <ol>\n        <li><b>Suggest_int:</b> Select an integer value within the range.</li>\n        <li><b>Suggest_uniform:</b> Select an equal distribution value within a range.</li>\n        <li><b>Suggest_discrete_uniform:</b> Select a discrete uniform distribution value within the range.</li>\n        <li><b>Suggest_loguniform:</b> Select a logarithmic function linear value within a range.</li>\n        <li><b>Suggest_category:</b> Select a value in the list.</li>\n    </ol>\n</p>","metadata":{}},{"cell_type":"code","source":"def objective_xgb(trial, X, y):\n    param = {\n        'n_estimators': 2000,\n        'max_depth': trial.suggest_int('max_depth', 3, 11),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.005, 0.01),\n        'subsample': trial.suggest_categorical('subsample', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n        'colsample_bylevel': trial.suggest_categorical('colsample_bylevel', [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 100),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 100),\n        'n_jobs': -1\n    }\n    train_scores, test_scores = [], []\n    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n    model = XGBRegressor(**param)\n    for train_idx, test_idx in kf.split(X):\n        tmp_X_train, tmp_X_test = X_train.iloc[train_idx, :], X_train.iloc[test_idx, :]\n        tmp_y_train, tmp_y_test = y_train[train_idx], y_train[test_idx]\n        model.fit(tmp_X_train, tmp_y_train,\n                 eval_set=[(tmp_X_test, tmp_y_test)], eval_metric=['rmse'],\n                 early_stopping_rounds=30, verbose=0,\n                 callbacks=[optuna.integration.XGBoostPruningCallback(trial, observation_key='validation_0-rmse')])\n        train_score = np.sqrt(mse(tmp_y_train, model.predict(tmp_X_train)))\n        test_score = np.sqrt(mse(tmp_y_test, model.predict(tmp_X_test)))\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    train_score = np.array(train_scores).mean()\n    test_score = np.array(test_scores).mean()\n    print(f'train score: {train_score}')\n    print(f'test score: {test_score}')\n    return test_score","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:34:39.006761Z","iopub.execute_input":"2021-10-23T18:34:39.007048Z","iopub.status.idle":"2021-10-23T18:34:39.022845Z","shell.execute_reply.started":"2021-10-23T18:34:39.007008Z","shell.execute_reply":"2021-10-23T18:34:39.021506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = partial(objective_xgb, X=X_train, y=y_train)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(optimizer, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T18:34:39.024557Z","iopub.execute_input":"2021-10-23T18:34:39.024929Z","iopub.status.idle":"2021-10-23T19:01:09.277178Z","shell.execute_reply.started":"2021-10-23T18:34:39.024866Z","shell.execute_reply":"2021-10-23T19:01:09.276446Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optuna can be used to visualize the optimal value for each trial or variable.","metadata":{}},{"cell_type":"code","source":"optuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:01:09.280954Z","iopub.execute_input":"2021-10-23T19:01:09.282702Z","iopub.status.idle":"2021-10-23T19:01:09.611929Z","shell.execute_reply.started":"2021-10-23T19:01:09.282661Z","shell.execute_reply":"2021-10-23T19:01:09.610919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(study)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:01:09.61332Z","iopub.execute_input":"2021-10-23T19:01:09.613569Z","iopub.status.idle":"2021-10-23T19:01:10.121416Z","shell.execute_reply.started":"2021-10-23T19:01:09.613541Z","shell.execute_reply":"2021-10-23T19:01:10.120552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params['XGBoost'] = study.best_params","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:01:10.123108Z","iopub.execute_input":"2021-10-23T19:01:10.123631Z","iopub.status.idle":"2021-10-23T19:01:10.133165Z","shell.execute_reply.started":"2021-10-23T19:01:10.123583Z","shell.execute_reply":"2021-10-23T19:01:10.132066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 style=\"text-align:center;\">Modeling</h2><br>","metadata":{}},{"cell_type":"markdown","source":"Create each model using the hyperparameter combination found earlier (Some parameters are corrected through several trials and errors).","metadata":{}},{"cell_type":"code","source":"best_params","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:01:10.136415Z","iopub.execute_input":"2021-10-23T19:01:10.137365Z","iopub.status.idle":"2021-10-23T19:01:10.146908Z","shell.execute_reply.started":"2021-10-23T19:01:10.13733Z","shell.execute_reply":"2021-10-23T19:01:10.146317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_lasso = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', Lasso(alpha=0.0005))\n])\n\nmodel_enet = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', ElasticNet(alpha=0.0089, l1_ratio=0.009000000000000001, random_state=3))\n])\n\nmodel_krr = Pipeline([\n    ('scaler', RobustScaler()),\n    ('model', krr(alpha=0.6,\n                        kernel='polynomial',\n                        degree=2,\n                        coef0=2.5))\n])\nmodel_xgbr = XGBRegressor(colsample_bytree=0.4, learning_rate=0.00898718134841855, max_depth=8, \n                             n_estimators=2200, reg_alpha=0.036142628805195254, reg_lambda=0.03188665185506858,\n                             subsample=0.6, random_state =42)\nmodel_gbr = gbr(n_estimators=3000, learning_rate=0.009995774699700678,\n                                   max_depth=8, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state=5)\nmodel_lgbm = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nstack_gen = StackingCVRegressor(regressors=(model_lgbm, model_lasso, model_enet, model_krr, model_gbr),\n                               meta_regressor=model_xgbr,\n                               use_features_in_secondary=True)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T22:39:34.386117Z","iopub.execute_input":"2021-10-23T22:39:34.386388Z","iopub.status.idle":"2021-10-23T22:39:34.39113Z","shell.execute_reply.started":"2021-10-23T22:39:34.386361Z","shell.execute_reply":"2021-10-23T22:39:34.390327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's check the cross-validation results of each model.","metadata":{}},{"cell_type":"code","source":"models = [\n    model_lasso, model_enet, model_krr, model_gbr, model_xgbr, model_lgbm\n]\ncross_score = {\n    'Lasso': 0,\n    'ElasticNet': 0,\n    'Kernel Ridge': 0,\n    'GradientBoosting': 0,\n    'XGBoost': 0,\n    'LightGBM': 0,\n}\n\nfor idx, model in enumerate(models):\n    cross_score[list(cross_score.keys())[idx]] = rmsle_cv(model).mean()","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:01:10.162468Z","iopub.execute_input":"2021-10-23T19:01:10.162833Z","iopub.status.idle":"2021-10-23T19:53:29.767093Z","shell.execute_reply.started":"2021-10-23T19:01:10.162807Z","shell.execute_reply":"2021-10-23T19:53:29.765621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cross_score","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:53:29.76978Z","iopub.execute_input":"2021-10-23T19:53:29.770335Z","iopub.status.idle":"2021-10-23T19:53:29.781737Z","shell.execute_reply.started":"2021-10-23T19:53:29.770287Z","shell.execute_reply":"2021-10-23T19:53:29.780921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"After creating a blend function that can harmonize the results of multiple models, traning each model","metadata":{}},{"cell_type":"code","source":"def blend(X):\n    return ((0.15 * model_lasso.predict(X)) + \\\n            (0.15 * model_enet.predict(X)) + \\\n            (0.05 * model_krr.predict(X)) + \\\n            (0.15 * model_xgbr.predict(X)) + \\\n            (0.15 * model_lgbm.predict(X)) + \\\n            (0.35 * stack_gen.predict(np.array(X))))","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:53:29.78453Z","iopub.execute_input":"2021-10-23T19:53:29.78494Z","iopub.status.idle":"2021-10-23T19:53:29.794032Z","shell.execute_reply.started":"2021-10-23T19:53:29.784884Z","shell.execute_reply":"2021-10-23T19:53:29.793187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for model in models:\n    model = model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:53:29.796471Z","iopub.execute_input":"2021-10-23T19:53:29.797077Z","iopub.status.idle":"2021-10-23T19:54:08.839687Z","shell.execute_reply.started":"2021-10-23T19:53:29.797032Z","shell.execute_reply":"2021-10-23T19:54:08.838948Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stack_gen = stack_gen.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:54:08.841209Z","iopub.execute_input":"2021-10-23T19:54:08.841685Z","iopub.status.idle":"2021-10-23T19:56:29.67999Z","shell.execute_reply.started":"2021-10-23T19:54:08.841645Z","shell.execute_reply":"2021-10-23T19:56:29.679174Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.sqrt(mse(y_train, blend(X_train)))","metadata":{"execution":{"iopub.status.busy":"2021-10-23T19:56:29.681364Z","iopub.execute_input":"2021-10-23T19:56:29.681749Z","iopub.status.idle":"2021-10-23T19:56:30.309134Z","shell.execute_reply.started":"2021-10-23T19:56:29.68172Z","shell.execute_reply":"2021-10-23T19:56:30.308444Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = score = np.expm1(blend(X_test))\nsub.to_csv('submission.csv',index=False)","metadata":{},"execution_count":null,"outputs":[]}]}